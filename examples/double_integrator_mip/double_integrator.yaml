# alpha, beta-CROWN parameters
general:
  device: cpu
  seed: 0
attack: # Currently attack is only implemented for Linf norm.
  pgd_steps: 100  # Increase for a stronger attack. A PGD attack will be used before verification to filter on non-robust data examples.
  pgd_restarts: 30  # Increase for a stronger attack.
solver:
  bound_prop_method: 'alpha-crown'
  batch_size: 2048  # Number of subdomains to compute in parallel in bound solver. Decrease if you run out of memory.
  alpha-crown:
    iteration: 100   # Number of iterations for alpha-CROWN optimization. Alpha-CROWN is used to compute all intermediate layer bounds before branch and bound starts.
    lr_alpha: 0.1    # Learning rate for alpha in alpha-CROWN. The default (0.1) is typically ok.
  beta-crown:
    lr_alpha: 0.01  # Learning rate for optimizing the alpha parameters, the default (0.01) is typically ok, but you can try to tune this parameter to get better lower bound.
    lr_beta: 0.05  # Learning rate for optimizing the beta parameters, the default (0.05) is typically ok, but you can try to tune this parameter to get better lower bound.
    iteration: 20  # Number of iterations for beta-CROWN optimization. 20 is often sufficient, 50 or 100 can also be used.
bab:
  get_upper_bound: True
  pruning_in_iteration: false
  timeout: 180 # Timeout threshold for branch and bound. Increase for verifying more points.
  branching: # Parameters for branching heuristics.
    reduceop: min  # Reduction function for the branching heuristic scores, min or max. Using max can be better on some models.
    method: kfsb  # babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usually a balance.
    candidates: 3  # Number of candidates to consider in fsb and kfsb. More leads to slower but better branching. 3 is typically good enough.

# options for training the barrier function
alg_options:
  train_method: fine-tuning # use 'verification-only' or 'fine-tuning'
  verification_method: mip # use 'bab' or 'mip'
  dynamics_fcn:
    train_nn_dynamics: False
  barrier_fcn:
    barrier_output_dim: 6
    dataset:
      collect_samples: False # collect samples to train the barrier function
      # number of samples to collect in order to train the barrier function
      num_samples_x0: 5000
      num_samples_xu: 5000
      num_samples_x: 30000
    train_options: # training parameters of the barrier function in one round
      num_epochs: 50
      l1_lambda: 0.00001 # l1 regularization parameter
      early_stopping_tol: 0.000000001
      update_A_freq: 1 # update the A matrix every x epochs
      samples_pool_size: 100000 # maximum number of training samples for each condition
      B_batch_size: 50 # batch size in training the barrier function
      num_iter: 1000 # number of the learn-verify iteration
      scaling_factor: 0.5 # scaling factor in the shrink-and-perturb strategy
      noise_weight: 1.0 # the noise weight in the shrink-and-perturb strategy
      train_timeout: 7200
#      condition_tol: 0.0001 # positivity or negativity tolerance for the vector barrier function conditions
      condition_tol: 0.00001 # positivity or negativity tolerance for the vector barrier function conditions
  ce_sampling: # options that generate additional samples in the neighborhood of a counterexample through gradient descent
    num_ce_samples: 50 # number of additional samples generated around one counterexample
    opt_iter: 100 # number of gradient descent steps
    radius: 0.1 # radius of ell_inf ball around each counterexample
    num_ce_samples_accpm: 10
  ACCPM:
    max_iter: 20
    cvxpy_solver: MOSEK
    num_ce_thresh: 10 # max. number of counterexamples added to the sample set in each iteration, should be >= barrier_output_dim
  bab:
    bab_yaml_path: 'double_integrator.yaml'
    adv_sample_filter_radius_x0: 0.1
    adv_sample_filter_radius_xu: 0.1
    adv_sample_filter_radius_x: 0.3
    adv_samples_pool_size: 50 # pool size in generating upper bounds in bab
    get_upper_bound_samples: 2048 # additional samples to refine the upper bound
  mip: # store mip solver options
    time_limit: 0.0 # 0.0 means no time limit
    MIPFocus: 0 # range: [0,1,2,3]. See guorbi webpage https://www.gurobi.com/documentation/current/refman/mipfocus.html for explanation



